[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "matching_separable_simuls",
    "section": "",
    "text": "pip install matching_separable_simuls"
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "matching_separable_simuls",
    "section": "summary",
    "text": "summary\nThe package reads the marriage patterns from the Choo and Siow 2006 Journal of Political Economy paper for the non-reform states of the 1970 wave. It fits a parsimonious homoskedastic logit model. Then it generates n_sim datasets from the fitted model, and it uses them to test the estimation methods presented in Galichon-SalaniÃ© 2022."
  },
  {
    "objectID": "index.html#setting-the-options",
    "href": "index.html#setting-the-options",
    "title": "matching_separable_simuls",
    "section": "setting the options",
    "text": "setting the options\n\ndo_simuls = False\nplot_simuls = False\n\n# model_name = \"choo_siow_firstsub\"\nmodel_name = \"choo_siow_cupid\"\n# model_name = \"choo_siow_firstsub10\"\n\nn_households_cupid_pop = 13_274_041    # number of households in the Cupid population\nn_households_cupid_obs = 75_265        # number of households in the Cupid sample \nn_households_sim = n_households_cupid_obs           # number of households in the simulation\nn_sim = 100                            # number of simulations\nvalue_coeff = 1.0                     # we set the zeros at the smallest positive value divided by value_coeff"
  },
  {
    "objectID": "index.html#using-the-cupidchoo-and-siow-data",
    "href": "index.html#using-the-cupidchoo-and-siow-data",
    "title": "matching_separable_simuls",
    "section": "using the Cupid/Choo and Siow data",
    "text": "using the Cupid/Choo and Siow data\nFirst, we read the Choo and Siow 1970 non-reform data used in Cupid.\n\nreading and prepping the data\n\n\n\nThe data has 25 types of men and 25 types of women.\n\n\nWe rescale the matching and we reshape and rescale the covariance matrix.\n\nif model_name == \"choo_siow_cupid\":\n    mus = Matching(muxy, nx, my)\n    mus_norm = rescale_mus(mus, n_households_cupid_obs)\n\n    varmus_norm = reshape_varcov(varmus, mus, n_households_cupid_obs)\n\n\nif model_name == \"choo_siow_cupid\":\n    mus = Matching(muxy, nx, my)\n    mus_norm = rescale_mus(mus, n_households_cupid_obs)\n\n    varmus_norm = reshape_varcov(varmus, mus, n_households_cupid_obs)\n\nFinally, we add a small number to zero cells to make them non-empty:\n\nif model_name == \"choo_siow_cupid\":\n    mus_norm_fixed = remove_zero_cells(mus_norm, coeff=value_coeff) \n    muxy_norm_fixed, mux0_norm_fixed, mu0y_norm_fixed, nx_norm_fixed, my_norm_fixed = mus_norm_fixed.unpack()\n\n\n\nsetting up a basic model\nWe need to generate some basis functions. First we quantile-transform the margins; then we generate orthogonal polynomials on [0,1]. Our first three base functions are 1, 1(x>y), and max(x-y, 0) (rescaled). Then we specify monomials \\(x^a y^b\\) in the degrees list.\n\nif model_name == \"choo_siow_cupid\":\n    degrees = [(1,0), (0,1), (2,0), (1,1), (0,2)]\n    base_functions, base_names = generate_bases(nx_norm_fixed, my_norm_fixed, degrees)\n    n_bases = base_functions.shape[-1]\n    print(f\"We created {n_bases} bases:\")\n    print(f\"{base_names}\")\n\nWe created 8 bases:\n['1', '1(x>y)', 'max(x-y,0)', 'x^1 * y^0', 'x^0 * y^1', 'x^2 * y^0', 'x^1 * y^1', 'x^0 * y^2']\n\n\n\n\nwe save the data and the bases we generated\n\nif model_name == \"choo_siow_cupid\":\n    with open(data_dir / f\"{model_name}_mus_norm_fixed_{int(value_coeff)}.pkl\", \"wb\") as f:\n        pickle.dump(mus_norm_fixed, f)\n    with open(data_dir / f\"{model_name}_varmus_norm.pkl\", \"wb\") as f:\n        pickle.dump(varmus_norm, f)\n    with open(data_dir / f\"{model_name}_base_functions.pkl\", \"wb\") as f:\n        pickle.dump(base_functions, f)\n\n\n\ntesting the estimation\n\nif model_name == \"choo_siow_cupid\":\n    mde_results = estimate_semilinear_mde(\n        mus_norm_fixed, base_functions, entropy_choo_siow, \n        more_params=None\n    )\n\n    estim_Phi = mde_results.estimated_Phi\n    estim_coeffs = mde_results.estimated_coefficients\n    varcov_coeffs = mde_results.varcov_coefficients\n    std_coeffs = mde_results.stderrs_coefficients\n\n    print(mde_results)\n    \n    with open(data_dir / f\"{model_name}_mde_results.pkl\", \"wb\") as f:\n        pickle.dump(mde_results, f)\n\n********************************************************************************\nThe data has 75264.82360878828 households\n\nThe model has 25x25 margins\n      The entropy is parameter-free. \nWe use 8 basis functions.\n\nThe estimated coefficients (and their standard errors) are\n\n   base 1:     -8.405 ( 0.114)\n   base 2:      1.355 ( 0.064)\n   base 3:    -17.184 ( 0.576)\n   base 4:     -4.236 ( 0.187)\n   base 5:      5.981 ( 0.166)\n   base 6:     -1.731 ( 0.112)\n   base 7:      2.620 ( 0.305)\n   base 8:     -0.836 ( 0.139)\n\nSpecification test:\n   the value of the test statistic is   2171.279\n     for a chi2(617), the p-value is      0.000\n********************************************************************************"
  },
  {
    "objectID": "index.html#generating-artificial-datasets-and-fitting-a-homoskedastic-logit-model",
    "href": "index.html#generating-artificial-datasets-and-fitting-a-homoskedastic-logit-model",
    "title": "matching_separable_simuls",
    "section": "generating artificial datasets and fitting a homoskedastic logit model",
    "text": "generating artificial datasets and fitting a homoskedastic logit model\n\nif model_name == \"choo_siow_cupid\":           # we use the Phi and the margins we got from the Cupid dataset\n    choo_siow_estim = ChooSiowPrimitives(estim_Phi, nx_norm_fixed, my_norm_fixed)\nelif model_name.startswith(\"choo_siow_firstsub\"):      # we regenerate the simulation in the first submitted version\n    n_types_men = n_types_women = 20\n    theta1 = np.array([1.0, 0.0, 0.0, -0.01, 0.02, -0.01, 0.5, 0.0])\n    if model_name == \"choo_siow_firstsub10\":\n        theta1 *= 10\n    n_bases = theta1.size\n    base_functions = np.zeros((n_types_men, n_types_women, n_bases))\n    base_functions[:, :, 0] = 1.0\n    vec_x = np.arange(n_types_men)\n    vec_y = np.arange(n_types_women)\n    base_functions[:, :, 1] = nprepeat_col(vec_x, n_types_women)\n    base_functions[:, :, 2] = nprepeat_row(vec_y, n_types_men)\n    base_functions[:, :, 3] = base_functions[:, :, 1] * base_functions[:, :, 1]\n    base_functions[:, :, 4] = base_functions[:, :, 1] * base_functions[:, :, 2]\n    base_functions[:, :, 5] = base_functions[:, :, 2] * base_functions[:, :, 2]\n    for i in range(n_types_men):\n        for j in range(i, n_types_women):\n            base_functions[i, j, 6] = 1\n            base_functions[i, j, 7] = i - j\n    base_names = [\"1\", \"x\", \"y\", \"x^2\", \"xy\", \"y^2\", \"1(x>y)\", \"max(x-y,0)\"]\n    Phi1 = base_functions @ theta1\n    t = 0.2\n    nx1 = np.logspace(start=0, base=1 - t, stop=n_types_men - 1, num=n_types_men)\n    my1 = np.logspace(start=0, base=1 - t, stop=n_types_women - 1, num=n_types_women)\n    choo_siow_estim = ChooSiowPrimitives(Phi1, nx1, my1)\n    estim_coeffs = theta1\n\n\ndef _run_simul(\n    i_sim: int,            # the index of the simulation\n    seed: int,             # the seed for its random draws \n    n_households_sim: float, # the number of households in the simulation\n    base_functions: np.ndarray,   # the bases\n    entropy: EntropyFunctions,  # the entropy \n    value_coeff: float          # the divider\n    ) ->  Tuple[MDEResults, PoissonGLMResults]:\n    \"\"\" runs one simulation for both estmators\"\"\"\n    mus_sim = choo_siow_estim.simulate(n_households_sim, seed=seed)\n    mus_sim_non0 = remove_zero_cells(mus_sim, coeff=value_coeff)\n    mde_results_sim = estimate_semilinear_mde(\n        mus_sim_non0, base_functions, entropy, more_params=None)\n    estim_coeffs_mde = mde_results_sim.estimated_coefficients\n    poisson_results_sim = choo_siow_poisson_glm(mus_sim_non0, base_functions, verbose = 0)\n    estim_coeffs_poisson = poisson_results_sim.estimated_beta\n    if i_sim % 10 == 0:\n        print(f\"Done simul {i_sim}\")\n    return estim_coeffs_mde, estim_coeffs_poisson\n\n\nif do_simuls:\n    \n    # generate random seeds\n    rng = np.random.default_rng(130962)\n    seeds = rng.integers(100_000, size=n_sim)\n\n    estim_coeffs_mde = np.zeros((n_sim, n_bases))\n    estim_coeffs_poisson = np.zeros((n_sim, n_bases))\n\n    list_args = [\n        [\n            i_sim, seeds[i_sim], n_households_sim, \n            base_functions, entropy_choo_siow, value_coeff\n        ] for i_sim in range(n_sim)\n    ]\n\n    nb_cpus = 8\n\n    with Pool(nb_cpus) as pool:\n        results = pool.starmap(_run_simul, list_args)\n\n    for i_sim in range(n_sim):\n        estim_coeffs_mde[i_sim, :] = results[i_sim][0]\n        estim_coeffs_poisson[i_sim, :] = results[i_sim][1]\n\n    simul_results = {'Base names': base_names, 'Base functions': base_functions,\n                     'True coeffs': estim_coeffs,\n                     'MDE': estim_coeffs_mde, 'Poisson': estim_coeffs_poisson}\n    \n    if model_name == \"choo_siow_cupid\":\n        simul_results['MDE stderrs'] = std_coeffs\n        simul_results['MDE varcov'] = varcov_coeffs\n\n    with open(data_dir / f\"{model_name}_{n_households_sim}_{int(value_coeff)}.pkl\", \"wb\") as f:\n        pickle.dump(simul_results, f)\n\nDone simul 0\nDone simul 20\nDone simul 30\nDone simul 10\nDone simul 40\nDone simul 60\nDone simul 50\nDone simul 70\nDone simul 80\nDone simul 90\n\n\n\nwe plot the distributions of the estimates\n\nn_households_obs = n_households_cupid_obs if model_name == \"choo_siow_cupid\" else None\n\nif plot_simuls:\n    plot_simulation_results(model_name, n_households_sim, value_coeff, n_households_obs)\n\nWe have a total of 0 outliers for MDE,  out of 100 simulations.\nWe have a total of 0 outliers for Poisson,  out of 100 simulations.\nWe have found no outlier samples"
  },
  {
    "objectID": "index.html#fitting-a-nested-logit-model",
    "href": "index.html#fitting-a-nested-logit-model",
    "title": "matching_separable_simuls",
    "section": "fitting a nested logit model",
    "text": "fitting a nested logit model"
  },
  {
    "objectID": "estimate.html",
    "href": "estimate.html",
    "title": "estimate",
    "section": "",
    "text": "source\n\ngenerate_bases\n\n generate_bases (nx:numpy.ndarray, my:numpy.ndarray,\n                 degrees:List[Tuple[int,int]])\n\ngenerates the bases for a semilinear specification\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnx\nndarray\nthe numbers of men of each type\n\n\nmy\nndarray\nthe numbers of women of each type\n\n\ndegrees\ntyping.List[typing.Tuple[int, int]]\nthe list of degrees for polynomials in x and y\n\n\nReturns\ntyping.Tuple[numpy.ndarray, typing.List[str]]\nthe matrix of base functions and their names"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nget_root_dir\n\n get_root_dir ()\n\nreturns the package directory\n\nsource\n\n\nnprepeat_row\n\n nprepeat_row (v:numpy.ndarray, m:int)\n\ncreate a matrix with m rows equal to v\n\n\n\n\nType\nDetails\n\n\n\n\nv\nndarray\na 1-dim array of size n\n\n\nm\nint\nthe number of rows requested\n\n\nReturns\nndarray\na 2-dim array of shape (m, n)\n\n\n\n\nsource\n\n\nnprepeat_col\n\n nprepeat_col (v:numpy.ndarray, n:int)\n\ncreate a matrix with n columns equal to the vectorv\n\n\n\n\nType\nDetails\n\n\n\n\nv\nndarray\na 1-dim array of size m\n\n\nn\nint\nthe number of columns requested\n\n\nReturns\nndarray\na 2-dim array of shape (m, n)\n\n\n\n\nsource\n\n\nlegendre_polynomials\n\n legendre_polynomials (x:numpy.ndarray, max_deg:int, a:float=-1.0,\n                       b:float=1.0, no_constant:bool=False)\n\nevaluates the Legendre polynomials over x in the interval [a, b]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\npoints where the polynomials are to be evaluated\n\n\nmax_deg\nint\n\nmaximum degree\n\n\na\nfloat\n-1.0\nstart of interval, classically -1\n\n\nb\nfloat\n1.0\nend of interval, classically 1\n\n\nno_constant\nbool\nFalse\nif True, delete the constant polynomial\n\n\nReturns\nndarray\n\nreturns an array of (max_deg+1) arrays of the shape of x\n\n\n\n\nsource\n\n\nquantile_transform\n\n quantile_transform (v:numpy.ndarray)\n\ntransform a vector of counts into the corresponding quantiles\n\n\n\n\nType\nDetails\n\n\n\n\nv\nndarray\na vector of counts\n\n\nReturns\nndarray\nthe corresponding quantiles"
  },
  {
    "objectID": "plots.html",
    "href": "plots.html",
    "title": "plots",
    "section": "",
    "text": "source\n\nplot_simulation_results\n\n plot_simulation_results (model_name:str, n_households_sim:float,\n                          value_coeff:float,\n                          n_households_cupid_obs:float=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_name\nstr\n\nthe type of model we are estimating\n\n\nn_households_sim\nfloat\n\nthe number of observed households in the simulation\n\n\nvalue_coeff\nfloat\n\nthe divider of the smallest positive mu\n\n\nn_households_cupid_obs\nfloat\nNone\nthe number of observed households in the Cupid dataset\n\n\nReturns\nNone"
  },
  {
    "objectID": "read_data.html",
    "href": "read_data.html",
    "title": "read_data",
    "section": "",
    "text": "source\n\nread_marriages\n\n read_marriages (data_dir:pathlib.Path)\n\nreads and returns the marriages and the variances\n\n\n\n\nType\nDetails\n\n\n\n\ndata_dir\nPath\nthe data directory\n\n\nReturns\ntyping.Tuple[numpy.ndarray, numpy.ndarray]\n\n\n\n\n\nsource\n\n\nread_margins\n\n read_margins (data_dir:pathlib.Path)\n\nreads and returns the margins for men and for women\n\n\n\n\nType\nDetails\n\n\n\n\ndata_dir\nPath\nthe data directory\n\n\nReturns\ntyping.Tuple[numpy.ndarray, numpy.ndarray]\n\n\n\n\n\nsource\n\n\nreshape_varcov\n\n reshape_varcov (varmus:numpy.ndarray,\n                 mus:cupid_matching.matching_utils.Matching,\n                 n_households:float)\n\nsplits the variance-covariance matrix and renomalizes for a requested total number of households\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nvarmus\nndarray\nmuxy row major, then mux0, then mu0y packed in both dimensions\n\n\nmus\nMatching\nthe original Matching\n\n\nn_households\nfloat\nthe number of households we want\n\n\nReturns\ntuple\nthe 6 constituent blocks of the normalized variance-covariance\n\n\n\n\nsource\n\n\nrescale_mus\n\n rescale_mus (mus:cupid_matching.matching_utils.Matching,\n              n_households:float)\n\nnormalizes the marriages and margins to a requested total number of households\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmus\nMatching\nmuxy, mux0, mu0y\n\n\nn_households\nfloat\nthe number of households we want\n\n\nReturns\nMatching\nthe normalized Matching after rescaling\n\n\n\n\nsource\n\n\nremove_zero_cells\n\n remove_zero_cells (mus:cupid_matching.matching_utils.Matching,\n                    coeff:float=100)\n\nadd small number delta to 0-cells to avoid numerical issues\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmus\nMatching\n\nmuxy, mux0, mu0y, n, m\n\n\ncoeff\nfloat\n100\ndefault scale factor for delta\n\n\nReturns\nMatching\n\nthe transformed muxy, mux0, mu0y, nx, my"
  }
]